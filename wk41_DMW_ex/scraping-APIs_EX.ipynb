{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d9270ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T07:04:24.659786Z",
     "iopub.status.busy": "2024-09-18T07:04:24.659352Z",
     "iopub.status.idle": "2024-09-18T07:04:25.517988Z",
     "shell.execute_reply": "2024-09-18T07:04:25.517289Z",
     "shell.execute_reply.started": "2024-09-18T07:04:24.659742Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# img = mpimg.imread('erdos.jpg')\n",
    "# imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0eeecc",
   "metadata": {},
   "source": [
    "## Background \n",
    "\n",
    "    Starting from any random Wikipedia page, following the first* page link of the current page will eventually lead you to the \"Philosophy\" page of Wikipedia (~95% of the time).\n",
    "     \n",
    "    * Due to the structure of Wikipedia articles, the first valid link must NOT:\n",
    "        \n",
    "        be in either parentheses (usually language pages) or italics (usually disambiguations)\n",
    "        \n",
    "        be a link to either a meta page, a page outside Wikipedia, or a broken link\n",
    "        \n",
    "        be an in-page citation\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852f5e4",
   "metadata": {},
   "source": [
    "## Basic Goal\n",
    "\n",
    "    Make a webcrawler that starts from a random Wikipedia page, and follows the first page link until it either finds the Philosophy page, a page with no links, or loops back to a previously visited link.\n",
    "    \n",
    "    Return the name of the starting page, and the degrees of separation from the Philosophy page (set degree to -1 if terminates otherwise)\n",
    "\n",
    "        Ex. the page \"Kevin Bacon\" should have a degree of separation of 12\n",
    "        \n",
    "    NOTE: For crawling, use the Wikipedia API. \n",
    "    \n",
    "    You can start from a random Wikipedia page with a URL (https://en.wikipedia.org/wiki/Special:Random)   \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ab187",
   "metadata": {},
   "source": [
    "## Advanced Goals\n",
    "\n",
    "    1.) Get the degrees of separation for 1000 random pages. Find the median degree of separation as well as the distribution.\n",
    "    \n",
    "    2.) Considering your results, and the size of the English Wikipedia, estimate how many pages there are of degree 6.\n",
    "    \n",
    "    3.) Try the Cebuano Wikipedia. Following the same rules above, determine the network structure. \n",
    "    Is there is a page a page that all others tend to lead back to? \n",
    "    Does it tend to loop more? \n",
    "    In any case, are the degrees generally shorter/longer?\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01c9d976",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T00:49:40.439828Z",
     "iopub.status.busy": "2024-10-15T00:49:40.439257Z",
     "iopub.status.idle": "2024-10-15T00:49:40.861715Z",
     "shell.execute_reply": "2024-10-15T00:49:40.860243Z",
     "shell.execute_reply.started": "2024-10-15T00:49:40.439781Z"
    }
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Enzo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Enzo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Enzo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:345\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Users\\Enzo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:363\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 14\u001b[0m\n\u001b[0;32m      7\u001b[0m PARAMS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitles\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrenada\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     13\u001b[0m R \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m.\u001b[39mget(url\u001b[38;5;241m=\u001b[39mURL, params\u001b[38;5;241m=\u001b[39mPARAMS)\n\u001b[1;32m---> 14\u001b[0m DATA \u001b[38;5;241m=\u001b[39m \u001b[43mR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(DATA)\n",
      "File \u001b[1;32mc:\\Users\\Enzo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"Grenada\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "\n",
    "print(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ac01c-25b4-48b8-a875-b7ed84cc66fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Random page error]: Expecting value: line 1 column 1 (char 0)\n",
      "[Random page error]: Expecting value: line 1 column 1 (char 0)\n",
      "[Random page error]: Expecting value: line 1 column 1 (char 0)\n",
      "[Random page error]: Expecting value: line 1 column 1 (char 0)\n",
      "[Random page error]: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "--- Summary Table ---\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'degree'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 147\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Summary Table ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m--> 147\u001b[0m valid_degrees \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdegree\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdegree\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_degrees\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m    149\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Enzo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Enzo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'degree'"
     ]
    }
   ],
   "source": [
    "# Wikipedia \"Philosophy\" Crawler (Final Fixed Version)\n",
    "# Author: [Your Name]\n",
    "# Course: Data Mining & Wrangling\n",
    "# Description: Starts from a Wikipedia page and follows the first valid link until reaching \"Philosophy\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Setup ===\n",
    "S = requests.Session()\n",
    "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "PHILOSOPHY_PAGE = \"Philosophy\"\n",
    "\n",
    "# === Function: Get HTML via Wikipedia API ===\n",
    "def get_page_html(title):\n",
    "    \"\"\"Fetch HTML content for a given Wikipedia page title using the API.\"\"\"\n",
    "    params = {\"action\": \"parse\", \"page\": title, \"format\": \"json\", \"prop\": \"text\"}\n",
    "    try:\n",
    "        response = S.get(url=API_URL, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        if response.text.strip() == \"\":\n",
    "            return None\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error fetching {title}]: {e}\")\n",
    "        return None\n",
    "\n",
    "    if \"error\" in data:\n",
    "        return None\n",
    "    return data[\"parse\"][\"text\"][\"*\"]\n",
    "\n",
    "# === Function: Extract the First Valid Wikipedia Link ===\n",
    "def get_first_valid_link_from_html(html):\n",
    "    \"\"\"\n",
    "    Extracts the first valid link from a Wikipedia HTML page.\n",
    "    Rules:\n",
    "      - Skip links in parentheses, italics, superscripts, or metadata.\n",
    "      - Ignore anchors, files, or help pages.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    content = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "    if not content:\n",
    "        return None\n",
    "\n",
    "    # Remove unwanted tags before parsing links\n",
    "    for tag in content.find_all([\"table\", \"i\", \"em\", \"sup\", \"span\", \"small\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Scan paragraph text\n",
    "    paragraphs = content.find_all(\"p\", recursive=True)\n",
    "    for p in paragraphs:\n",
    "        if not p.text.strip():\n",
    "            continue\n",
    "\n",
    "        # Remove text inside parentheses\n",
    "        text_no_parens = re.sub(r\"\\([^()]*\\)\", \"\", p.text)\n",
    "\n",
    "        for a in p.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "\n",
    "            # Must be a valid internal wiki link\n",
    "            if not href.startswith(\"/wiki/\"):\n",
    "                continue\n",
    "\n",
    "            # Skip meta or anchor links\n",
    "            if any(prefix in href for prefix in [\":\", \"#\"]):\n",
    "                continue\n",
    "\n",
    "            # Skip if link is in italics or superscripts\n",
    "            if any(parent.name in [\"i\", \"em\", \"sup\", \"small\"] for parent in a.parents):\n",
    "                continue\n",
    "\n",
    "            # First valid link found\n",
    "            return href.split(\"/wiki/\")[-1]\n",
    "\n",
    "    return None\n",
    "\n",
    "# === Function: Follow the First Link Chain ===\n",
    "def follow_to_philosophy(start_title=\"Grenada\", max_steps=100):\n",
    "    visited = set()\n",
    "    current_title = start_title\n",
    "    steps = 0\n",
    "\n",
    "    while steps < max_steps:\n",
    "        if current_title in visited:\n",
    "            return start_title, -1, \"Loop detected\"\n",
    "        visited.add(current_title)\n",
    "\n",
    "        print(f\"Step {steps}: {current_title}\")\n",
    "\n",
    "        if current_title.lower() == PHILOSOPHY_PAGE.lower():\n",
    "            return start_title, steps, \"Reached Philosophy\"\n",
    "\n",
    "        html = get_page_html(current_title)\n",
    "        if not html:\n",
    "            return start_title, -1, \"Page fetch failed\"\n",
    "\n",
    "        next_title = get_first_valid_link_from_html(html)\n",
    "        if not next_title:\n",
    "            return start_title, -1, \"No valid links found\"\n",
    "\n",
    "        current_title = next_title\n",
    "        steps += 1\n",
    "        time.sleep(0.6)  # polite delay to avoid throttling\n",
    "\n",
    "    return start_title, -1, \"Exceeded max steps\"\n",
    "\n",
    "# === Function: Get a Random Wikipedia Page Title ===\n",
    "def get_random_wikipedia_title():\n",
    "    \"\"\"Returns a random article title from Wikipedia.\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"random\",\n",
    "        \"rnnamespace\": 0,\n",
    "        \"rnlimit\": 1\n",
    "    }\n",
    "    try:\n",
    "        r = S.get(API_URL, params=params, timeout=10)\n",
    "        data = r.json()\n",
    "        return data[\"query\"][\"random\"][0][\"title\"]\n",
    "    except Exception as e:\n",
    "        print(f\"[Random page error]: {e}\")\n",
    "        return None\n",
    "\n",
    "# === Main Execution ===\n",
    "num_pages = 5  # Reduce to 5 for testing; can increase later\n",
    "results = []\n",
    "\n",
    "for i in range(num_pages):\n",
    "    title = get_random_wikipedia_title()\n",
    "    if not title:\n",
    "        continue\n",
    "    print(f\"\\n=== Random Page {i+1}/{num_pages}: {title} ===\")\n",
    "    start, degree, status = follow_to_philosophy(title)\n",
    "    results.append({\"start_page\": start, \"degree\": degree, \"status\": status})\n",
    "\n",
    "# === Results DataFrame & Visualization ===\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n--- Summary Table ---\")\n",
    "print(df)\n",
    "\n",
    "valid_degrees = df[df[\"degree\"] >= 0][\"degree\"]\n",
    "if not valid_degrees.empty:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(valid_degrees, bins=10, edgecolor=\"black\")\n",
    "    plt.title(\"Distribution of Degrees to Reach Philosophy\")\n",
    "    plt.xlabel(\"Degrees of Separation\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    print(f\"\\nMedian Degrees of Separation: {valid_degrees.median()}\")\n",
    "else:\n",
    "    print(\"\\nNo valid degree data available.\")\n",
    "\n",
    "print(\"\\nStatus counts:\\n\", df[\"status\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d61062c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\"?><api batchcomplete=\"\"><query><normalized><n from=\"Emu_War\" to=\"Emu War\" /></normalized><pages><page _idx=\"1274780\" pageid=\"1274780\" ns=\"0\" title=\"Emu War\"><links><pl ns=\"0\" title=\"1933 Western Australian secession referendum\" /><pl ns=\"0\" title=\"Ambush\" /><pl ns=\"0\" title=\"Australia\" /><pl ns=\"0\" title=\"Australian Army\" /><pl ns=\"0\" title=\"Australian Light Horse\" /><pl ns=\"0\" title=\"Australian Senate\" /><pl ns=\"0\" title=\"Bibcode (identifier)\" /><pl ns=\"0\" title=\"Birds of Western Australia (book)\" /><pl ns=\"0\" title=\"Brumby shooting\" /><pl ns=\"0\" title=\"Campion, Western Australia\" /><pl ns=\"0\" title=\"Chandler, Western Australia\" /><pl ns=\"0\" title=\"Cinematographer\" /><pl ns=\"0\" title=\"Coolgardie Miner\" /><pl ns=\"0\" title=\"Cornell University Press\" /><pl ns=\"0\" title=\"Dam\" /><pl ns=\"0\" title=\"Dingo\" /><pl ns=\"0\" title=\"Dingo Fence\" /><pl ns=\"0\" title=\"Doi (identifier)\" /><pl ns=\"0\" title=\"Dominic Serventy\" /><pl ns=\"0\" title=\"Emu\" /><pl ns=\"0\" title=\"Expanding bullet\" /><pl ns=\"0\" title=\"Fallow\" /><pl ns=\"0\" title=\"Flightless bird\" /><pl ns=\"0\" title=\"Four Pests campaign\" /><pl ns=\"0\" title=\"Frontiers in Ecology and the Environment\" /><pl ns=\"0\" title=\"George Pearce\" /><pl ns=\"0\" title=\"Great Depression in Australia\" /><pl ns=\"0\" title=\"House of Representatives (Australia)\" /><pl ns=\"0\" title=\"Hubert Massey Whittell\" /><pl ns=\"0\" title=\"Hubert Whittell\" /><pl ns=\"0\" title=\"Hugh Leslie\" /><pl ns=\"0\" title=\"ISBN (identifier)\" /><pl ns=\"0\" title=\"ISSN (identifier)\" /><pl ns=\"0\" title=\"JSTOR (identifier)\" /><pl ns=\"0\" title=\"James Dunn (Australian politician)\" /><pl ns=\"0\" title=\"James Mitchell (Australian politician)\" /><pl ns=\"0\" title=\"Jim Jefferies\" /><pl ns=\"0\" title=\"John Cleese\" /><pl ns=\"0\" title=\"Joseph Lyons\" /><pl ns=\"0\" title=\"Josiah Francis\" /><pl ns=\"0\" title=\"Lang Labor\" /><pl ns=\"0\" title=\"Lewis gun\" /><pl ns=\"0\" title=\"Machine gun\" /><pl ns=\"0\" title=\"Military discharge\" /><pl ns=\"0\" title=\"Military operation\" /><pl ns=\"0\" title=\"Minister for Defence (Australia)\" /><pl ns=\"0\" title=\"Monster Fest\" /><pl ns=\"0\" title=\"Movietone News\" /><pl ns=\"0\" title=\"New South Wales\" /><pl ns=\"0\" title=\"Nuisance wildlife management\" /><pl ns=\"0\" title=\"Pest-exclusion fence\" /><pl ns=\"0\" title=\"Premier of Western Australia\" /><pl ns=\"0\" title=\"Question time\" /><pl ns=\"0\" title=\"Rabbit\" /><pl ns=\"0\" title=\"Rabbits in Australia\" /><pl ns=\"0\" title=\"Rob Schneider\" /><pl ns=\"0\" title=\"Rowley James\" /><pl ns=\"0\" title=\"Royal Australian Artillery\" /><pl ns=\"0\" title=\"S2CID (identifier)\" /><pl ns=\"0\" title=\"Soldier settlement (Australia)\" /><pl ns=\"0\" title=\"Subsidies\" /><pl ns=\"0\" title=\"Sydney\" /><pl ns=\"0\" title=\"The Argus (Melbourne)\" /><pl ns=\"0\" title=\"The Canberra Times\" /><pl ns=\"0\" title=\"The Sun-Herald\" /><pl ns=\"0\" title=\"Trove\" /><pl ns=\"0\" title=\"University of Michigan Press\" /><pl ns=\"0\" title=\"University of Wisconsin Press\" /><pl ns=\"0\" title=\"Vermin\" /><pl ns=\"0\" title=\"Veteran\" /><pl ns=\"0\" title=\"Walgoolan, Western Australia\" /><pl ns=\"0\" title=\"Western Australia\" /><pl ns=\"0\" title=\"Wheat\" /><pl ns=\"0\" title=\"Wheatbelt (Western Australia)\" /><pl ns=\"0\" title=\"World War I\" /><pl ns=\"0\" title=\"Zulu people\" /><pl ns=\"4\" title=\"Wikipedia:Citing sources\" /><pl ns=\"4\" title=\"Wikipedia:Manual of Style/Dates and numbers\" /><pl ns=\"4\" title=\"Wikipedia:Protection policy\" /></links></page></pages></query></api>'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'JOJIE-jbautista'\n",
    "}\n",
    "\n",
    "requests.get(\n",
    "    'https://en.wikipedia.org/w/api.php',\n",
    "    params={\n",
    "        'action': 'query',\n",
    "        'prop': 'links',\n",
    "        'titles': 'Emu_War',\n",
    "        'pllimit': 500,\n",
    "        'format': 'xml',\n",
    "    },\n",
    "    headers=headers\n",
    ").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1eae2f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Linked Pages\n",
      "0       39th Santa Barbara International Film Festival\n",
      "1       40th Santa Barbara International Film Festival\n",
      "2                                             8 (play)\n",
      "3                                       A Few Good Men\n",
      "4                                A Few Good Men (film)\n",
      "..                                                 ...\n",
      "328  Template talk:Golden Globe Best Actor TV Minis...\n",
      "329                          Template talk:Kevin Bacon\n",
      "330  Template talk:Saturn Award for Best Actor on T...\n",
      "331  Template talk:ScreenActorsGuildAward MaleTVMin...\n",
      "332                             Help:Authority control\n",
      "\n",
      "[333 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "ac14f126-459c-4c6b-974a-fd5923998d8a",
       "rows": [
        [
         "0",
         "b'{\"batchcomplete\":\"\",\"query\":{\"normalized\":[{\"from\":\"Kevin_Bacon\",\"to\":\"Kevin Bacon\"}],\"pages\":{\"16827\":{\"pageid\":16827,\"ns\":0,\"t'"
        ],
        [
         "1",
         "b'itle\":\"Kevin Bacon\",\"links\":[{\"ns\":0,\"title\":\"39th Santa Barbara International Film Festival\"},{\"ns\":0,\"title\":\"40th Santa Barba'"
        ],
        [
         "2",
         "b'ra International Film Festival\"},{\"ns\":0,\"title\":\"8 (play)\"},{\"ns\":0,\"title\":\"A Few Good Men\"},{\"ns\":0,\"title\":\"A Few Good Men ('"
        ],
        [
         "3",
         "b'film)\"},{\"ns\":0,\"title\":\"Access Hollywood\"},{\"ns\":0,\"title\":\"Adrien Brody\"},{\"ns\":0,\"title\":\"Al Pacino\"},{\"ns\":0,\"title\":\"Alan R'"
        ],
        [
         "4",
         "b'ickman\"},{\"ns\":0,\"title\":\"Albert Finney\"},{\"ns\":0,\"title\":\"Alexander Skarsg\\\\u00e5rd\"},{\"ns\":0,\"title\":\"Amazon Prime Video\"},{\"ns'"
        ],
        [
         "5",
         "b'\":0,\"title\":\"American Foundation for Equal Rights\"},{\"ns\":0,\"title\":\"Andrew Lincoln\"},{\"ns\":0,\"title\":\"Animal House\"},{\"ns\":0,\"t'"
        ],
        [
         "6",
         "b'itle\":\"Annette Bening\"},{\"ns\":0,\"title\":\"Anthony Andrews\"},{\"ns\":0,\"title\":\"Antireligion\"},{\"ns\":0,\"title\":\"Apollo 13 (film)\"},{'"
        ],
        [
         "7",
         "b'\"ns\":0,\"title\":\"Atheism\"},{\"ns\":0,\"title\":\"Atom Egoyan\"},{\"ns\":0,\"title\":\"Balto (film)\"},{\"ns\":0,\"title\":\"Barack Obama\"},{\"ns\":0'"
        ],
        [
         "8",
         "b',\"title\":\"Barry Levinson\"},{\"ns\":0,\"title\":\"Beau Bridges\"},{\"ns\":0,\"title\":\"Beauty Shop\"},{\"ns\":0,\"title\":\"Ben Browder\"},{\"ns\":0'"
        ],
        [
         "9",
         "b',\"title\":\"Ben Kingsley\"},{\"ns\":0,\"title\":\"Bernie Madoff\"},{\"ns\":0,\"title\":\"Big screen\"},{\"ns\":0,\"title\":\"Bill Nighy\"},{\"ns\":0,\"t'"
        ],
        [
         "10",
         "b'itle\":\"Billy Bob Thornton\"},{\"ns\":0,\"title\":\"Birth name\"},{\"ns\":0,\"title\":\"Black Mass (film)\"},{\"ns\":0,\"title\":\"Blockbuster (ent'"
        ],
        [
         "11",
         "b'ertainment)\"},{\"ns\":0,\"title\":\"Blockbuster Entertainment Awards\"},{\"ns\":0,\"title\":\"Bob Odenkirk\"},{\"ns\":0,\"title\":\"Boston Marath'"
        ],
        [
         "12",
         "b'on bombing\"},{\"ns\":0,\"title\":\"Boston Society of Film Critics\"},{\"ns\":0,\"title\":\"Boston Society of Film Critics Award for Best Ca'"
        ],
        [
         "13",
         "b'st\"},{\"ns\":0,\"title\":\"Bravo Otto\"},{\"ns\":0,\"title\":\"Brendan Fraser\"},{\"ns\":0,\"title\":\"Brian Dennehy\"},{\"ns\":0,\"title\":\"Broadcast'"
        ],
        [
         "14",
         "b' Film Critics Association\"},{\"ns\":0,\"title\":\"Bruce Campbell\"},{\"ns\":0,\"title\":\"Bryan Cranston\"},{\"ns\":0,\"title\":\"Bucknell Univer'"
        ],
        [
         "15",
         "b'sity\"},{\"ns\":0,\"title\":\"CNN\"},{\"ns\":0,\"title\":\"CableACE Award\"},{\"ns\":0,\"title\":\"Cannes Film Festival\"},{\"ns\":0,\"title\":\"Casey A'"
        ],
        [
         "16",
         "b'ffleck\"},{\"ns\":0,\"title\":\"Chadwick Boseman\"},{\"ns\":0,\"title\":\"Charles J. Cooper\"},{\"ns\":0,\"title\":\"Chlotrudis Awards\"},{\"ns\":0,\"'"
        ],
        [
         "17",
         "b'title\":\"Christian Bale\"},{\"ns\":0,\"title\":\"Christopher Reeve\"},{\"ns\":0,\"title\":\"Circle in the Square\"},{\"ns\":0,\"title\":\"City on a'"
        ],
        [
         "18",
         "b' Hill (TV series)\"},{\"ns\":0,\"title\":\"Clint Eastwood\"},{\"ns\":0,\"title\":\"Colin Farrell\"},{\"ns\":0,\"title\":\"Colin Firth\"},{\"ns\":0,\"t'"
        ],
        [
         "19",
         "b'itle\":\"Cosmopolitan (magazine)\"},{\"ns\":0,\"title\":\"Crazy, Stupid, Love\"},{\"ns\":0,\"title\":\"Critics\\' Choice Movie Award for Best Ac'"
        ],
        [
         "20",
         "b'tor\"},{\"ns\":0,\"title\":\"Critics\\' Choice Movie Awards\"},{\"ns\":0,\"title\":\"Daniel Day-Lewis\"},{\"ns\":0,\"title\":\"Daniel Stern (actor)\"'"
        ],
        [
         "21",
         "b'},{\"ns\":0,\"title\":\"Darren Criss\"},{\"ns\":0,\"title\":\"David Boreanaz\"},{\"ns\":0,\"title\":\"David Koepp\"},{\"ns\":0,\"title\":\"Delroy Lindo'"
        ],
        [
         "22",
         "b'\"},{\"ns\":0,\"title\":\"Denver Film Festival\"},{\"ns\":0,\"title\":\"Desert Storm\"},{\"ns\":0,\"title\":\"Design of Cities\"},{\"ns\":0,\"title\":\"'"
        ],
        [
         "23",
         "b'Diane Lane\"},{\"ns\":0,\"title\":\"Digging to China\"},{\"ns\":0,\"title\":\"Diner (1982 film)\"},{\"ns\":0,\"title\":\"Dustin Hoffman\"},{\"ns\":0,'"
        ],
        [
         "24",
         "b'\"title\":\"Dustin Lance Black\"},{\"ns\":0,\"title\":\"E! People\\'s Choice Awards\"},{\"ns\":0,\"title\":\"EE (telecommunications)\"},{\"ns\":0,\"t'"
        ],
        [
         "25",
         "b'itle\":\"Edmund Bacon (architect)\"},{\"ns\":0,\"title\":\"Edward James Olmos\"},{\"ns\":0,\"title\":\"Elizabeth Perkins\"},{\"ns\":0,\"title\":\"El'"
        ],
        [
         "26",
         "b'len Barkin\"},{\"ns\":0,\"title\":\"Erd\\\\u0151s number\"},{\"ns\":0,\"title\":\"Erd\\\\u0151s\\\\u2013Bacon number\"},{\"ns\":0,\"title\":\"Ethan Hawke\"}'"
        ],
        [
         "27",
         "b',{\"ns\":0,\"title\":\"Evan Peters\"},{\"ns\":0,\"title\":\"Ewan McGregor\"},{\"ns\":0,\"title\":\"Finding Your Roots\"},{\"ns\":0,\"title\":\"Flatline'"
        ],
        [
         "28",
         "b'rs\"},{\"ns\":0,\"title\":\"Flux\"},{\"ns\":0,\"title\":\"Footloose (1984 film)\"},{\"ns\":0,\"title\":\"Forest Whitaker\"},{\"ns\":0,\"title\":\"Forty '"
        ],
        [
         "29",
         "b'Deuce\"},{\"ns\":0,\"title\":\"Fox Broadcasting Company\"},{\"ns\":0,\"title\":\"Fraternities and sororities\"},{\"ns\":0,\"title\":\"Friday the 1'"
        ],
        [
         "30",
         "b'3th (1980 film)\"},{\"ns\":0,\"title\":\"Frost/Nixon (film)\"},{\"ns\":0,\"title\":\"Gary Oldman\"},{\"ns\":0,\"title\":\"Gary Sinise\"},{\"ns\":0,\"t'"
        ],
        [
         "31",
         "b'itle\":\"Geoffrey Rush\"},{\"ns\":0,\"title\":\"George Clooney\"},{\"ns\":0,\"title\":\"Getting Out\"},{\"ns\":0,\"title\":\"Ghent International Fil'"
        ],
        [
         "32",
         "b'm Festival\"},{\"ns\":0,\"title\":\"Giffoni Film Festival\"},{\"ns\":0,\"title\":\"Glory Van Scott\"},{\"ns\":0,\"title\":\"Golden Globe Award\"},{'"
        ],
        [
         "33",
         "b'\"ns\":0,\"title\":\"Golden Globe Award for Best Actor \\\\u2013 Miniseries or Television Film\"},{\"ns\":0,\"title\":\"Golden Globe Award for'"
        ],
        [
         "34",
         "b' Best Actor \\\\u2013 Television Series Musical or Comedy\"},{\"ns\":0,\"title\":\"Golden Globe Award for Best Supporting Actor \\\\u2013 Mo'"
        ],
        [
         "35",
         "b'tion Picture\"},{\"ns\":0,\"title\":\"Guiding Light\"},{\"ns\":0,\"title\":\"HBO\"},{\"ns\":0,\"title\":\"HBO Films\"},{\"ns\":0,\"title\":\"He Said, Sh'"
        ],
        [
         "36",
         "b'e Said (film)\"},{\"ns\":0,\"title\":\"Henry Louis Gates\"},{\"ns\":0,\"title\":\"Henry Thomas\"},{\"ns\":0,\"title\":\"Hollow Man\"},{\"ns\":0,\"titl'"
        ],
        [
         "37",
         "b'e\":\"Hollywood Walk of Fame\"},{\"ns\":0,\"title\":\"Huffington Post\"},{\"ns\":0,\"title\":\"Hugh Dancy\"},{\"ns\":0,\"title\":\"IMDb\"},{\"ns\":0,\"t'"
        ],
        [
         "38",
         "b'itle\":\"IMDb (identifier)\"},{\"ns\":0,\"title\":\"I Love Dick (TV series)\"},{\"ns\":0,\"title\":\"Ian McKellen\"},{\"ns\":0,\"title\":\"Idris Elb'"
        ],
        [
         "39",
         "b'a\"},{\"ns\":0,\"title\":\"Independent Spirit Award for Best Male Lead\"},{\"ns\":0,\"title\":\"Independent Spirit Awards\"},{\"ns\":0,\"title\":'"
        ],
        [
         "40",
         "b'\"Instagram\"},{\"ns\":0,\"title\":\"Internet Broadway Database\"},{\"ns\":0,\"title\":\"Internet Off-Broadway Database\"},{\"ns\":0,\"title\":\"In'"
        ],
        [
         "41",
         "b'ternet meme\"},{\"ns\":0,\"title\":\"It\\'s a New Day (Will.i.am song)\"},{\"ns\":0,\"title\":\"JFK (film)\"},{\"ns\":0,\"title\":\"Jack Lemmon\"},{\"'"
        ],
        [
         "42",
         "b'ns\":0,\"title\":\"Jack Nicholson\"},{\"ns\":0,\"title\":\"James Dean\"},{\"ns\":0,\"title\":\"James Franco\"},{\"ns\":0,\"title\":\"James Garner\"},{\"'"
        ],
        [
         "43",
         "b'ns\":0,\"title\":\"James Woods\"},{\"ns\":0,\"title\":\"Jamie Foxx\"},{\"ns\":0,\"title\":\"Jeff Bridges\"},{\"ns\":0,\"title\":\"Jeremy Irons\"},{\"ns\"'"
        ],
        [
         "44",
         "b':0,\"title\":\"Jim Broadbent\"},{\"ns\":0,\"title\":\"Joaquin Phoenix\"},{\"ns\":0,\"title\":\"Joel Schumacher\"},{\"ns\":0,\"title\":\"John Hughes ('"
        ],
        [
         "45",
         "b'filmmaker)\"},{\"ns\":0,\"title\":\"Johnny Depp\"},{\"ns\":0,\"title\":\"Jonathan Rhys Meyers\"},{\"ns\":0,\"title\":\"Josh Holloway\"},{\"ns\":0,\"ti'"
        ],
        [
         "46",
         "b'tle\":\"Judy Garland\"},{\"ns\":0,\"title\":\"Julia R. Masterman School\"},{\"ns\":0,\"title\":\"Kevin Bacon (disambiguation)\"},{\"ns\":0,\"title'"
        ],
        [
         "47",
         "b'\":\"Kevin Bacon filmography\"},{\"ns\":0,\"title\":\"Kevin Costner\"},{\"ns\":0,\"title\":\"Kevin Kline\"},{\"ns\":0,\"title\":\"Kristen Stewart\"},'"
        ],
        [
         "48",
         "b'{\"ns\":0,\"title\":\"Kyle Chandler\"},{\"ns\":0,\"title\":\"Kyle MacLachlan\"},{\"ns\":0,\"title\":\"Kyra Sedgwick\"},{\"ns\":0,\"title\":\"Lanford Wi'"
        ],
        [
         "49",
         "b'lson\"},{\"ns\":0,\"title\":\"Leading man\"},{\"ns\":0,\"title\":\"Lemon Sky\"},{\"ns\":0,\"title\":\"Leonardo DiCaprio\"},{\"ns\":0,\"title\":\"Lewisbu'"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 106
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'{\"batchcomplete\":\"\",\"query\":{\"normalized\":[{...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'itle\":\"Kevin Bacon\",\"links\":[{\"ns\":0,\"title\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'ra International Film Festival\"},{\"ns\":0,\"ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'film)\"},{\"ns\":0,\"title\":\"Access Hollywood\"},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'ickman\"},{\"ns\":0,\"title\":\"Albert Finney\"},{\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>b'te:Saturn Award for Best Actor on Television...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>b'1,\"title\":\"Template talk:American Riviera Aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>b'\"ns\":11,\"title\":\"Template talk:Golden Globe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>b'11,\"title\":\"Template talk:Saturn Award for B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>b'eTVMiniseriesMovie\"},{\"ns\":12,\"title\":\"Help:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0    b'{\"batchcomplete\":\"\",\"query\":{\"normalized\":[{...\n",
       "1    b'itle\":\"Kevin Bacon\",\"links\":[{\"ns\":0,\"title\"...\n",
       "2    b'ra International Film Festival\"},{\"ns\":0,\"ti...\n",
       "3    b'film)\"},{\"ns\":0,\"title\":\"Access Hollywood\"},...\n",
       "4    b'ickman\"},{\"ns\":0,\"title\":\"Albert Finney\"},{\"...\n",
       "..                                                 ...\n",
       "101  b'te:Saturn Award for Best Actor on Television...\n",
       "102  b'1,\"title\":\"Template talk:American Riviera Aw...\n",
       "103  b'\"ns\":11,\"title\":\"Template talk:Golden Globe ...\n",
       "104  b'11,\"title\":\"Template talk:Saturn Award for B...\n",
       "105  b'eTVMiniseriesMovie\"},{\"ns\":12,\"title\":\"Help:...\n",
       "\n",
       "[106 rows x 1 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"JOJIE-jbautista\"\n",
    "}\n",
    "\n",
    "def normalize_wiki_target(target: str) -> str:\n",
    "    \"\"\"\n",
    "    Accept either a full Wikipedia article URL or a bare title and\n",
    "    return the API-ready page title.\n",
    "    \"\"\"\n",
    "    if target.lower().startswith(\"http\"):\n",
    "        parsed = urlparse(target)\n",
    "        if parsed.netloc.endswith(\"wikipedia.org\"):\n",
    "            path = parsed.path  # e.g. /wiki/Emu_War\n",
    "            if path.startswith(\"/wiki/\") and len(path) > len(\"/wiki/\"):\n",
    "                return path.split(\"/wiki/\", 1)[1]\n",
    "    return target.replace(\" \", \"_\")\n",
    "\n",
    "# Choose the page to inspect: plain title or full URL both work\n",
    "page_title = normalize_wiki_target(\"https://en.wikipedia.org/wiki/Emu_War\")\n",
    "page_title = normalize_wiki_target(\"https://en.wikipedia.org/wiki/Kevin_Bacon\")\n",
    "# page_title = normalize_wiki_target(\"Emu War\")\n",
    "\n",
    "params = {\n",
    "    \"action\": \"query\",\n",
    "    \"prop\": \"links\",\n",
    "    \"titles\": page_title,\n",
    "    \"pllimit\": 500,\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "response = requests.get(\"https://en.wikipedia.org/w/api.php\", params=params, headers=headers)\n",
    "\n",
    "if response.status_code == 200 and \"application/json\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "    data = response.json()\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    all_links = [\n",
    "        link[\"title\"]\n",
    "        for page_info in pages.values()\n",
    "        if \"links\" in page_info\n",
    "        for link in page_info[\"links\"]\n",
    "    ]\n",
    "    df = pd.DataFrame(all_links, columns=[\"Linked Pages\"])\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Error: Could not get valid JSON response\")\n",
    "    print(\"Response Code:\", response.status_code)\n",
    "    print(\"Response Content:\", response.text[:500])\n",
    "\n",
    "df = pd.DataFrame(response)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11e279",
   "metadata": {},
   "source": [
    "Working Script but does not hit Philosophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d036217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting from: 15_Again\n",
      "Step 0: 15_Again\n",
      "Step 1: Electronica\n",
      "Step 2: Electronic_music\n",
      "Step 3: Music_genre\n",
      "Step 4: Music\n",
      "No valid links found — dead end.\n",
      "\n",
      "Result: Started from '15_Again', Degrees of Separation = -1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# --- Session setup with a custom User-Agent (important for Wikipedia API) ---\n",
    "S = requests.Session()\n",
    "S.headers.update({'User-Agent': 'JOJIE-jbautista'})\n",
    "\n",
    "BASE_URL = \"https://en.wikipedia.org\"\n",
    "PHILOSOPHY_PAGE = \"Philosophy\"\n",
    "\n",
    "# --- Get a random Wikipedia page title ---\n",
    "def get_random_page_title():\n",
    "    resp = S.get(\"https://en.wikipedia.org/wiki/Special:Random\", allow_redirects=True)\n",
    "    # resp = S.get(\"https://en.wikipedia.org/wiki/Animal\", allow_redirects=True)\n",
    "    return resp.url.split(\"/wiki/\")[-1]\n",
    "\n",
    "# --- Get first valid link from a Wikipedia page ---\n",
    "def get_first_valid_link(title):\n",
    "    url = f\"{BASE_URL}/wiki/{title}\"\n",
    "    resp = S.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    content = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "    if not content:\n",
    "        return None\n",
    "\n",
    "    # Clean up: remove tables, italics, and small metadata sections\n",
    "    for tag in content.find_all([\"table\", \"i\", \"em\", \"span\", \"sup\", \"small\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Find first valid link in a paragraph\n",
    "    for paragraph in content.find_all(\"p\", recursive=True):\n",
    "        if not paragraph.text.strip():\n",
    "            continue\n",
    "\n",
    "        # Ignore text inside parentheses\n",
    "        for a in paragraph.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            # Must be a valid wiki article link (not Help:, File:, #, etc.)\n",
    "            if not href.startswith(\"/wiki/\") or any(prefix in href for prefix in [\":\", \"#\"]):\n",
    "                continue\n",
    "\n",
    "            # Skip links inside italics or superscripts\n",
    "            if any(parent.name in [\"i\", \"em\", \"sup\", \"small\"] for parent in a.parents):\n",
    "                continue\n",
    "\n",
    "            return href.split(\"/wiki/\")[-1]\n",
    "    return None\n",
    "\n",
    "# --- Main crawler function ---\n",
    "def follow_to_philosophy(start_title=None, max_steps=100):\n",
    "    if not start_title:\n",
    "        start_title = get_random_page_title()\n",
    "\n",
    "    visited = set()\n",
    "    current_title = start_title\n",
    "    steps = 0\n",
    "\n",
    "    print(f\"\\nStarting from: {current_title}\")\n",
    "\n",
    "    while steps < max_steps:\n",
    "        if current_title in visited:\n",
    "            print(\"Loop detected!\")\n",
    "            return start_title, -1\n",
    "        visited.add(current_title)\n",
    "\n",
    "        print(f\"Step {steps}: {current_title}\")\n",
    "        if current_title.lower() == PHILOSOPHY_PAGE.lower():\n",
    "            print(\"Reached Philosophy!\")\n",
    "            return start_title, steps\n",
    "\n",
    "        next_title = get_first_valid_link(current_title)\n",
    "        if not next_title:\n",
    "            print(\"No valid links found — dead end.\")\n",
    "            return start_title, -1\n",
    "\n",
    "        current_title = next_title\n",
    "        steps += 1\n",
    "        time.sleep(0.5)  # polite delay\n",
    "\n",
    "    print(\"Exceeded max steps.\")\n",
    "    return start_title, -1\n",
    "\n",
    "# --- Run a single test ---\n",
    "start, degree = follow_to_philosophy()\n",
    "print(f\"\\nResult: Started from '{start}', Degrees of Separation = {degree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb91abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting from: Whale\n",
      "Step 0: Whale\n",
      "First link found from Whale → Aquatic_animal\n",
      "Step 1: Aquatic_animal\n",
      "Step 2: Animal\n",
      "Step 3: Multicellular\n",
      "Step 4: Organism\n",
      "Step 5: Life\n",
      "Step 6: Matter\n",
      "Step 7: Mass\n",
      "Step 8: Analytical_mechanics\n",
      "Step 9: Lagrangian_mechanics\n",
      "Loop detected!\n",
      "\n",
      "Result: Started from 'Whale', Degrees of Separation = -1\n",
      "Full Path:\n",
      "Whale → Aquatic_animal → Animal → Multicellular → Organism → Life → Matter → Mass → Analytical_mechanics → Lagrangian_mechanics → Analytical_mechanics\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "\n",
    "# --- Session setup ---\n",
    "S = requests.Session()\n",
    "S.headers.update({'User-Agent': 'JOJIE-jbautista'})\n",
    "\n",
    "BASE_URL = \"https://en.wikipedia.org\"\n",
    "PHILOSOPHY_PAGE = \"Philosophy\"\n",
    "\n",
    "# --- Always start from Kevin Bacon ---\n",
    "def get_random_page_title():\n",
    "    resp = S.get(\"https://en.wikipedia.org/wiki/Kevin_Bacon\", allow_redirects=True)\n",
    "    resp = S.get(\"https://en.wikipedia.org/wiki/Whale\", allow_redirects=True)\n",
    "    # resp = S.get(\"https://en.wikipedia.org/wiki/Animal\", allow_redirects=True)\n",
    "    return resp.url.split(\"/wiki/\")[-1]\n",
    "\n",
    "# --- Regex patterns ---\n",
    "# Basic pattern for <a href=\"/wiki/...\">text</a>\n",
    "link_pattern = re.compile(\n",
    "    r'<a href=\"(/wiki/[^\"#:]+)\"[^>]*>(.*?)</a>',\n",
    "    re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "\n",
    "# Pattern to strip HTML tags (for text-only cleanup)\n",
    "tag_cleaner = re.compile(r'<[^>]+>')\n",
    "\n",
    "# --- Find first valid link in HTML using regex ---\n",
    "def get_first_valid_link_regex(html: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Return the first valid /wiki/... link in the article HTML,\n",
    "    starting after the bolded name (</b>), skipping <figcaption> sections,\n",
    "    italicized links, and ignoring links inside parentheses.\n",
    "    \"\"\"\n",
    "\n",
    "    # Focus only on the main article body\n",
    "    m = re.search(r'<div id=\"mw-content-text\"[^>]*>(.*?)<div id=\"mw-navigation\"', html, flags=re.DOTALL)\n",
    "    if m:\n",
    "        html = m.group(1)\n",
    "\n",
    "    # 🔹 Start parsing after the bolded name (</b>)\n",
    "    marker_index = html.find(\"</b>\")\n",
    "    if marker_index != -1:\n",
    "        html = html[marker_index + len(\"</b>\"):]\n",
    "\n",
    "    # 🔹 Remove all <figcaption>...</figcaption> sections\n",
    "    html = re.sub(r'<figcaption.*?</figcaption>', '', html, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # 🔹 Remove all italicized sections (<i>...</i> and <em>...</em>)\n",
    "    html = re.sub(r'<i.*?</i>', '', html, flags=re.DOTALL | re.IGNORECASE)\n",
    "    html = re.sub(r'<em.*?</em>', '', html, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Limit to lead section (before first <h2>)\n",
    "    lead_section = html.split(\"<h2\")[0]\n",
    "\n",
    "    # Remove citation superscripts\n",
    "    lead_section = re.sub(r'<sup.*?</sup>', '', lead_section, flags=re.DOTALL)\n",
    "\n",
    "    paren_depth = 0\n",
    "    pos = 0\n",
    "\n",
    "    for match in link_pattern.finditer(lead_section):\n",
    "        href, text = match.groups()\n",
    "        before = lead_section[pos:match.start()]\n",
    "        pos = match.end()\n",
    "\n",
    "        # Track parentheses nesting\n",
    "        for ch in before:\n",
    "            if ch == \"(\":\n",
    "                paren_depth += 1\n",
    "            elif ch == \")\":\n",
    "                paren_depth = max(paren_depth - 1, 0)\n",
    "\n",
    "        # Only consider links outside parentheses\n",
    "        if paren_depth == 0:\n",
    "            clean_text = tag_cleaner.sub('', text).strip()\n",
    "            if clean_text:\n",
    "                # Skip irrelevant or self links\n",
    "                if \"Main_Page\" in href or href.lower().endswith(\"kevin_bacon\"):\n",
    "                    continue\n",
    "                return href.split(\"/wiki/\")[-1]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Get first valid link from a Wikipedia title ---\n",
    "def get_first_valid_link(title: str) -> str | None:\n",
    "    url = f\"{BASE_URL}/wiki/{title}\"\n",
    "    r = S.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"[HTTP {r.status_code}] {url}\")\n",
    "        return None\n",
    "\n",
    "    html = r.text\n",
    "    link = get_first_valid_link_regex(html)\n",
    "    return link\n",
    "\n",
    "# --- Follow links until reaching Philosophy ---\n",
    "def follow_to_philosophy(start_title=None, max_steps=100):\n",
    "    if not start_title:\n",
    "        start_title = get_random_page_title()\n",
    "\n",
    "    visited = set()\n",
    "    current = start_title\n",
    "    steps = 0\n",
    "    path = [current]\n",
    "\n",
    "    print(f\"\\nStarting from: {current}\")\n",
    "\n",
    "    while steps < max_steps:\n",
    "        if current in visited:\n",
    "            print(\"Loop detected!\")\n",
    "            return start_title, -1, path\n",
    "        visited.add(current)\n",
    "\n",
    "        print(f\"Step {steps}: {current}\")\n",
    "        if current.lower() == PHILOSOPHY_PAGE.lower():\n",
    "            print(\"Reached Philosophy!\")\n",
    "            return start_title, steps, path\n",
    "\n",
    "        next_title = get_first_valid_link(current)\n",
    "        if not next_title:\n",
    "            print(\"No valid links found — dead end.\")\n",
    "            return start_title, -1, path\n",
    "\n",
    "        if steps == 0:\n",
    "            print(f\"First link found from {current} → {next_title}\")\n",
    "\n",
    "        path.append(next_title)\n",
    "        current = next_title\n",
    "        steps += 1\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    print(\"Exceeded max steps.\")\n",
    "    return start_title, -1, path\n",
    "\n",
    "# --- Run the crawler ---\n",
    "start_title = get_random_page_title()\n",
    "start, degree, path = follow_to_philosophy(start_title)\n",
    "\n",
    "print(f\"\\nResult: Started from '{start}', Degrees of Separation = {degree}\")\n",
    "print(\"Full Path:\")\n",
    "print(\" → \".join(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622405e8",
   "metadata": {},
   "source": [
    "Random Slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7e1189b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting from: Kevin_Bacon\n",
      "Step 0: Kevin_Bacon\n",
      "No valid links found — dead end.\n",
      "\n",
      "Result: Started from 'Kevin_Bacon', Degrees of Separation = -1\n",
      "Full Path:\n",
      "Kevin_Bacon\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "\n",
    "# --- Session setup ---\n",
    "S = requests.Session()\n",
    "S.headers.update({'User-Agent': 'JOJIE-jbautista'})\n",
    "\n",
    "BASE_URL = \"https://en.wikipedia.org\"\n",
    "PHILOSOPHY_PAGE = \"Philosophy\"\n",
    "\n",
    "# --- Always start from Kevin Bacon ---\n",
    "def get_random_page_title():\n",
    "    resp = S.get(\"https://en.wikipedia.org/wiki/Kevin_Bacon\", allow_redirects=True)\n",
    "    return resp.url.split(\"/wiki/\")[-1]\n",
    "\n",
    "# --- Regex patterns ---\n",
    "# Basic pattern for <a href=\"/wiki/...\">text</a>\n",
    "link_pattern = re.compile(\n",
    "    r'<a href=\"(/wiki/[^\"#:]+)\"[^>]*>(.*?)</a>',\n",
    "    re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "\n",
    "# Pattern to strip HTML tags (for text-only cleanup)\n",
    "tag_cleaner = re.compile(r'<[^>]+>')\n",
    "\n",
    "# --- Find first valid link in HTML using regex ---\n",
    "def get_first_valid_link_regex(html: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Return the first valid /wiki/... link in the lead section of a Wikipedia article,\n",
    "    skipping links inside parentheses.\n",
    "    \"\"\"\n",
    "    # Focus only on the article content (ignore navigation/sidebar/header)\n",
    "    m = re.search(r'<div id=\"mw-content-text\"[^>]*>(.*?)<div id=\"mw-navigation\"', html, flags=re.DOTALL)\n",
    "    if m:\n",
    "        html = m.group(1)\n",
    "    else:\n",
    "        # fallback: if pattern not found, use entire HTML\n",
    "        pass\n",
    "\n",
    "    # Limit search to lead section (before first <h2>)\n",
    "    lead_section = html.split(\"<h2\")[0]\n",
    "\n",
    "    # Remove citation superscripts\n",
    "    lead_section = re.sub(r'<sup.*?</sup>', '', lead_section, flags=re.DOTALL)\n",
    "\n",
    "    paren_depth = 0\n",
    "    pos = 0\n",
    "\n",
    "    for match in link_pattern.finditer(lead_section):\n",
    "        href, text = match.groups()\n",
    "        before = lead_section[pos:match.start()]\n",
    "        pos = match.end()\n",
    "\n",
    "        for ch in before:\n",
    "            if ch == \"(\":\n",
    "                paren_depth += 1\n",
    "            elif ch == \")\":\n",
    "                paren_depth = max(paren_depth - 1, 0)\n",
    "\n",
    "        if paren_depth == 0:\n",
    "            clean_text = tag_cleaner.sub('', text).strip()\n",
    "            if clean_text:\n",
    "                # skip Main_Page or other irrelevant links\n",
    "                if href.endswith(\"Main_Page\"):\n",
    "                    continue\n",
    "                return href.split(\"/wiki/\")[-1]\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Get first valid link from a Wikipedia title ---\n",
    "def get_first_valid_link(title: str) -> str | None:\n",
    "    url = f\"{BASE_URL}/wiki/{title}\"\n",
    "    r = S.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"[HTTP {r.status_code}] {url}\")\n",
    "        return None\n",
    "\n",
    "    html = r.text\n",
    "    link = get_first_valid_link_regex(html)\n",
    "    return link\n",
    "\n",
    "# --- Follow links until reaching Philosophy ---\n",
    "def follow_to_philosophy(start_title=None, max_steps=100):\n",
    "    if not start_title:\n",
    "        start_title = get_random_page_title()\n",
    "\n",
    "    visited = set()\n",
    "    current = start_title\n",
    "    steps = 0\n",
    "    path = [current]\n",
    "\n",
    "    print(f\"\\nStarting from: {current}\")\n",
    "\n",
    "    while steps < max_steps:\n",
    "        if current in visited:\n",
    "            print(\"Loop detected!\")\n",
    "            return start_title, -1, path\n",
    "        visited.add(current)\n",
    "\n",
    "        print(f\"Step {steps}: {current}\")\n",
    "        if current.lower() == PHILOSOPHY_PAGE.lower():\n",
    "            print(\"Reached Philosophy!\")\n",
    "            return start_title, steps, path\n",
    "\n",
    "        next_title = get_first_valid_link(current)\n",
    "        if not next_title:\n",
    "            print(\"No valid links found — dead end.\")\n",
    "            return start_title, -1, path\n",
    "\n",
    "        if steps == 0:\n",
    "            print(f\"First link found from {current} → {next_title}\")\n",
    "\n",
    "        path.append(next_title)\n",
    "        current = next_title\n",
    "        steps += 1\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    print(\"Exceeded max steps.\")\n",
    "    return start_title, -1, path\n",
    "\n",
    "# --- Run the crawler ---\n",
    "start_title = get_random_page_title()\n",
    "start, degree, path = follow_to_philosophy(start_title)\n",
    "\n",
    "print(f\"\\nResult: Started from '{start}', Degrees of Separation = {degree}\")\n",
    "print(\"Full Path:\")\n",
    "print(\" → \".join(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7eca2c",
   "metadata": {},
   "source": [
    "Random slop 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ed759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting from: Kevin_Bacon\n",
      "Step 0: Kevin_Bacon\n",
      "No valid links found — dead end.\n",
      "\n",
      "Result: Started from 'Kevin_Bacon', Degrees of Separation = -1\n",
      "Full Path:\n",
      "Kevin_Bacon\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "S = requests.Session()\n",
    "S.headers.update({'User-Agent': 'JOJIE-jbautista'})\n",
    "\n",
    "BASE_URL = \"https://en.wikipedia.org\"\n",
    "PHILOSOPHY_PAGE = \"Philosophy\"\n",
    "\n",
    "def get_random_page_title():\n",
    "    resp = S.get(\"https://en.wikipedia.org/wiki/Kevin_Bacon\", allow_redirects=True)\n",
    "    return resp.url.split(\"/wiki/\")[-1]\n",
    "\n",
    "def get_first_valid_link_regex(html: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Return the first valid /wiki/... link in the HTML,\n",
    "    starting only after the first <tbody> and skipping links inside parentheses.\n",
    "    \"\"\"\n",
    "    # Find the position after the first <tbody>\n",
    "    tbody_index = html.find(\"<tbody>\")\n",
    "    if tbody_index != -1:\n",
    "        html = html[tbody_index:]  # Trim everything before <tbody>\n",
    "\n",
    "    # Remove citation superscripts\n",
    "    html = re.sub(r'<sup.*?</sup>', '', html, flags=re.DOTALL)\n",
    "\n",
    "    # Limit to content before first <h2> if you still want to stop at lead section\n",
    "    lead_section = html.split(\"<h2\")[0]\n",
    "\n",
    "    paren_depth = 0\n",
    "    pos = 0\n",
    "\n",
    "    for match in link_pattern.finditer(lead_section):\n",
    "        href, text = match.groups()\n",
    "        before = lead_section[pos:match.start()]\n",
    "        pos = match.end()\n",
    "\n",
    "        # Track parentheses nesting\n",
    "        for ch in before:\n",
    "            if ch == \"(\":\n",
    "                paren_depth += 1\n",
    "            elif ch == \")\":\n",
    "                paren_depth = max(paren_depth - 1, 0)\n",
    "\n",
    "        if paren_depth == 0:\n",
    "            clean_text = tag_cleaner.sub('', text).strip()\n",
    "            if clean_text:\n",
    "                # skip links to Main_Page or Help/Portal/Filespaces\n",
    "                if any(x in href for x in [\"Main_Page\", \"Help:\", \"File:\", \"Portal:\"]):\n",
    "                    continue\n",
    "                return href.split(\"/wiki/\")[-1]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def follow_to_philosophy(start_title=None, max_steps=100):\n",
    "    if not start_title:\n",
    "        start_title = get_random_page_title()\n",
    "\n",
    "    visited = set()\n",
    "    current = start_title\n",
    "    steps = 0\n",
    "    path = [current]\n",
    "\n",
    "    print(f\"\\nStarting from: {current}\")\n",
    "\n",
    "    while steps < max_steps:\n",
    "        if current in visited:\n",
    "            print(\"Loop detected!\")\n",
    "            return start_title, -1, path\n",
    "        visited.add(current)\n",
    "\n",
    "        print(f\"Step {steps}: {current}\")\n",
    "        if current.lower() == PHILOSOPHY_PAGE.lower():\n",
    "            print(\"Reached Philosophy!\")\n",
    "            return start_title, steps, path\n",
    "\n",
    "        next_title = get_first_valid_link(current)\n",
    "        if not next_title:\n",
    "            print(\"No valid links found — dead end.\")\n",
    "            return start_title, -1, path\n",
    "\n",
    "        if steps == 0:\n",
    "            print(f\"First link found from {current} → {next_title}\")\n",
    "\n",
    "        path.append(next_title)\n",
    "        current = next_title\n",
    "        steps += 1\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    print(\"Exceeded max steps.\")\n",
    "    return start_title, -1, path\n",
    "\n",
    "# --- Run the crawler ---\n",
    "start_title = get_random_page_title()\n",
    "start, degree, path = follow_to_philosophy(start_title)\n",
    "\n",
    "print(f\"\\nResult: Started from '{start}', Degrees of Separation = {degree}\")\n",
    "print(\"Full Path:\")\n",
    "print(\" → \".join(path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
