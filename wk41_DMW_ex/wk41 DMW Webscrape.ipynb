{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Title / Notes (Markdown)\n",
    "# Use a Markdown cell with:\n",
    "# # Wikipedia Philosophy Crawler\n",
    "# Follow the first valid link rule until reaching Philosophy, looping, or stalling.\n",
    "# Requires the packages: `requests`, `beautifulsoup4`.\n",
    "\n",
    "# Cell 2\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import urllib.parse\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Set, Tuple\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "PHILOSOPHY_TITLE = \"Philosophy\"\n",
    "\n",
    "\n",
    "class WikipediaAPIError(RuntimeError):\n",
    "    \"\"\"Raised when the Wikipedia API returns an unexpected response.\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CrawlResult:\n",
    "    start_title: str\n",
    "    degree_of_separation: int\n",
    "    path: Tuple[str, ...]\n",
    "    termination_reason: str\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        lines = [\n",
    "            f\"Starting page: {self.start_title}\",\n",
    "            f\"Degree of separation: {self.degree_of_separation}\",\n",
    "            f\"Termination reason: {self.termination_reason}\",\n",
    "        ]\n",
    "        return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb64f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "def fetch_random_title(session: Optional[requests.Session] = None) -> str:\n",
    "    \"\"\"Return the title of a random article in the main namespace.\"\"\"\n",
    "    session = session or requests.Session()\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"random\",\n",
    "        \"rnnamespace\": 0,\n",
    "        \"rnlimit\": 1,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    response = session.get(API_URL, params=params, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    random_pages = data.get(\"query\", {}).get(\"random\")\n",
    "    if not random_pages:\n",
    "        raise WikipediaAPIError(\"No random page returned by the Wikipedia API.\")\n",
    "    return random_pages[0][\"title\"]\n",
    "\n",
    "\n",
    "def fetch_page_html(title: str, session: Optional[requests.Session] = None) -> str:\n",
    "    \"\"\"Return the rendered HTML for the given article title.\"\"\"\n",
    "    session = session or requests.Session()\n",
    "    params = {\n",
    "        \"action\": \"parse\",\n",
    "        \"page\": title,\n",
    "        \"prop\": \"text\",\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\": 2,\n",
    "    }\n",
    "    response = session.get(API_URL, params=params, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    if \"error\" in data:\n",
    "        raise WikipediaAPIError(data[\"error\"].get(\"info\", \"Unknown API error.\"))\n",
    "    parse_data = data.get(\"parse\")\n",
    "    if not parse_data or \"text\" not in parse_data:\n",
    "        raise WikipediaAPIError(f\"Missing page content for '{title}'.\")\n",
    "    return parse_data[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "def is_valid_link(tag: Tag) -> bool:\n",
    "    \"\"\"Return True if the anchor tag is a valid article link to follow.\"\"\"\n",
    "    if tag.name != \"a\":\n",
    "        return False\n",
    "\n",
    "    href = tag.get(\"href\")\n",
    "    if not href or not href.startswith(\"/wiki/\"):\n",
    "        return False\n",
    "\n",
    "    target = href.split(\"/wiki/\", 1)[1]\n",
    "    if \":\" in target or \"#\" in target:\n",
    "        return False\n",
    "\n",
    "    classes = tag.get(\"class\", [])\n",
    "    if \"new\" in classes or \"mw-selflink\" in classes:\n",
    "        return False\n",
    "\n",
    "    for parent in tag.parents:\n",
    "        if parent.name in (\"i\", \"em\"):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_first_link(html: str) -> Optional[str]:\n",
    "    \"\"\"Return the title of the first valid link in the article body.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    content = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "    if not content:\n",
    "        return None\n",
    "\n",
    "    def traverse(node, depth: int) -> Tuple[Optional[str], int]:\n",
    "        for child in node.children:\n",
    "            if isinstance(child, NavigableString):\n",
    "                for char in str(child):\n",
    "                    if char == \"(\":\n",
    "                        depth += 1\n",
    "                    elif char == \")\":\n",
    "                        depth = max(depth - 1, 0)\n",
    "                continue\n",
    "\n",
    "            if isinstance(child, Tag):\n",
    "                if child.name == \"a\" and depth == 0 and is_valid_link(child):\n",
    "                    return child[\"href\"], depth\n",
    "\n",
    "                if child.name == \"sup\" and \"reference\" in child.get(\"class\", []):\n",
    "                    continue\n",
    "\n",
    "                candidate, depth = traverse(child, depth)\n",
    "                if candidate:\n",
    "                    return candidate, depth\n",
    "        return None, depth\n",
    "\n",
    "    depth = 0\n",
    "    for section in content.find_all([\"p\", \"ul\", \"ol\"], recursive=False):\n",
    "        candidate, depth = traverse(section, depth)\n",
    "        if candidate:\n",
    "            decoded = urllib.parse.unquote(candidate.split(\"/wiki/\", 1)[1])\n",
    "            return decoded.replace(\"_\", \" \")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d78d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "def crawl_to_philosophy(\n",
    "    start_title: Optional[str] = None,\n",
    "    max_steps: int = 100,\n",
    "    delay: float = 0.1,\n",
    "    session: Optional[requests.Session] = None,\n",
    ") -> CrawlResult:\n",
    "    \"\"\"Follow the first valid link on successive pages until Philosophy or termination.\"\"\"\n",
    "    session = session or requests.Session()\n",
    "    current_title = start_title or fetch_random_title(session=session)\n",
    "    visited: Set[str] = set()\n",
    "    path = [current_title]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        if current_title == PHILOSOPHY_TITLE:\n",
    "            return CrawlResult(\n",
    "                start_title=path[0],\n",
    "                degree_of_separation=len(path) - 1,\n",
    "                path=tuple(path),\n",
    "                termination_reason=\"Reached Philosophy\",\n",
    "            )\n",
    "\n",
    "        if current_title in visited:\n",
    "            return CrawlResult(\n",
    "                start_title=path[0],\n",
    "                degree_of_separation=-1,\n",
    "                path=tuple(path),\n",
    "                termination_reason=f\"Detected loop at '{current_title}'.\",\n",
    "            )\n",
    "\n",
    "        visited.add(current_title)\n",
    "\n",
    "        try:\n",
    "            html = fetch_page_html(current_title, session=session)\n",
    "        except (requests.RequestException, WikipediaAPIError) as exc:\n",
    "            return CrawlResult(\n",
    "                start_title=path[0],\n",
    "                degree_of_separation=-1,\n",
    "                path=tuple(path),\n",
    "                termination_reason=f\"Failed to fetch '{current_title}': {exc}\",\n",
    "            )\n",
    "\n",
    "        next_title = extract_first_link(html)\n",
    "        if not next_title:\n",
    "            return CrawlResult(\n",
    "                start_title=path[0],\n",
    "                degree_of_separation=-1,\n",
    "                path=tuple(path),\n",
    "                termination_reason=f\"No valid links on '{current_title}'.\",\n",
    "            )\n",
    "\n",
    "        current_title = next_title\n",
    "        path.append(current_title)\n",
    "\n",
    "        if delay:\n",
    "            time.sleep(delay)\n",
    "\n",
    "    return CrawlResult(\n",
    "        start_title=path[0],\n",
    "        degree_of_separation=-1,\n",
    "        path=tuple(path),\n",
    "        termination_reason=f\"Maximum step count ({max_steps}) exceeded.\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69927ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - Example usage (runs real HTTP requests)\n",
    "session = requests.Session()\n",
    "result = crawl_to_philosophy(start_title=\"Kevin Bacon\", session=session, delay=0.0)\n",
    "print(result)\n",
    "\n",
    "print(\"\\nTraversal path (first 15 nodes):\")\n",
    "for index, page in enumerate(result.path[:15]):\n",
    "    print(f\"{index:>2}: {page}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
